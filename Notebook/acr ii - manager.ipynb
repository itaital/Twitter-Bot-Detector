{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os as os\n",
    "import io\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Reshape, Embedding, Flatten, Conv1D, Conv2D, MaxPooling1D ,MaxPooling2D, Activation\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "bots_list= []\n",
    "max_input_lenght = 0\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split()] #if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(raw_predictions, label_encoder):\n",
    "    # convert raw predictions to class indexes\n",
    "    threshold = 0.5\n",
    "    class_predictions = [(x > threshold).astype(int) for x in model.predict(x_test)]\n",
    "\n",
    "    # convert raw predictions to class indexes\n",
    "    threshold = 0.5\n",
    "    class_predictions = [(x > threshold).astype(int) for x in model.predict(x_test)]\n",
    "\n",
    "    # select only one class (i.e., the dim in the vector with 1.0 all other are at 0.0)\n",
    "    class_index = ([np.argmax(x) for x in class_predictions])\n",
    "\n",
    "    # convert back to original class names\n",
    "    pred_classes = label_encoder.inverse_transform(class_index)\n",
    "\n",
    "    # print precision, recall, f1-score report\n",
    "    print(classification_report(y_test, pred_classes))\n",
    "\n",
    "def load_fasttext_embeddings():\n",
    "    glove_dir = os.getcwd()\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'wiki-news-300d-1M.vec'), encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def create_embeddings_matrix(embeddings_index, vocabulary, embedding_dim=100):\n",
    "    embeddings_matrix = np.random.rand(len(vocabulary)+1, embedding_dim)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "    print('Matrix shape: {}'.format(embeddings_matrix.shape))\n",
    "    return embeddings_matrix\n",
    "\n",
    "\n",
    "def get_embeddings_layer(embeddings_matrix, name, max_len, trainable=False):\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=embeddings_matrix.shape[0],\n",
    "        output_dim=embeddings_matrix.shape[1],\n",
    "        input_length=max_len,\n",
    "        weights=[embeddings_matrix],\n",
    "        trainable=trainable,\n",
    "        name=name)\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def get_conv_pool_arc_II(x_input, sufix,maxlen, n_grams=[3,4,5], feature_maps=100): #maybe we need to change feature_maps = 500\n",
    "    branches = []\n",
    "    kernel_size_1d = 3\n",
    "    num_conv2d_layers = 2\n",
    "    filters_2d=[256,128]\n",
    "    kernel_size_2d=[[3,3], [3,3]]\n",
    "    mpool_size_2d=[[2,2], [2,2]]\n",
    "    dropout_rate=0\n",
    "    \n",
    "    layer1_conv = Conv1D(filters=maxlen, kernel_size=kernel_size_1d, padding='same')(x_input)\n",
    "    layer1_activation=Activation('relu')(layer1_conv)\n",
    "    print(\"layer1_activation:\")\n",
    "    print(layer1_activation.shape)\n",
    "    layer1_reshaped=Reshape((maxlen, maxlen, -1))(layer1_activation)\n",
    "    z=MaxPooling2D(pool_size=(2,2))(layer1_reshaped)\n",
    "    #z=MaxPooling2D(pool_size=(2,2))(layer1_activation)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z=Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same')(z)\n",
    "        z=Activation('relu')(z)\n",
    "        z=MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "    \n",
    "    pool1_flat=Flatten()(z)\n",
    "    pool1_flat_drop=Dropout(rate=dropout_rate)(pool1_flat)\n",
    "    pool1_norm=BatchNormalization()(pool1_flat_drop)\n",
    "    mlp1=Dense(64)(pool1_norm)\n",
    "    mlp1=Activation('relu')(mlp1)\n",
    "\n",
    "    return mlp1\n",
    "def get_cnn_pre_trained_embeddings(embedding_layer, max_len):\n",
    "    # connect the input with the embedding layer\n",
    "    input_1 = Input(shape=(max_len,), dtype='int32', name='input_1')\n",
    "    input_2 = Input(shape=(max_len,), dtype='int32', name='input_2')\n",
    "    x_1 = embedding_layer(input_1)\n",
    "    x_2 = embedding_layer(input_2)\n",
    "    \n",
    "    layer1_input=concatenate([x_1, x_2])\n",
    "\n",
    "    # generate several branches in the network, each for a different convolution+pooling operation,\n",
    "    # and concatenate the result of each branch into a single vector\n",
    "    mlp1 = get_conv_pool_arc_II(layer1_input, 'static',max_len)\n",
    "\n",
    "    # pass the concatenated vector to the predition layer\n",
    "    o = Dense(2, activation='sigmoid', name='output')(mlp1)\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=o)\n",
    "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build test and train files\n",
    "don't take time - prepare dataset: open train and test X and Y files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Tokenizer(text_for_tokenizer):\n",
    "    global tokenizer\n",
    "    tokenizer.fit_on_texts(text_for_tokenizer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Train_Set(X_train, y_train):\n",
    "    global bots_list\n",
    "    global max_input_lenght\n",
    "    global tokenizer\n",
    "    \n",
    "    # built two lists with tweets and labels\n",
    "    tweets_train = [x for x in X_train]\n",
    "    labels_train = [y for y in y_train]\n",
    "\n",
    "    # convert list of tokens/words to indexes\n",
    "    sequences_train = tokenizer.texts_to_sequences(tweets_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # get the max sentence lenght, needed for padding\n",
    "    max_input_lenght = max([len(x) for x in sequences_train])\n",
    "    print(\"Max. sequence lenght: \", max_input_lenght)\n",
    "\n",
    "    # pad all the sequences of indexes to the 'max_input_lenght'\n",
    "    data_train = pad_sequences(sequences_train, maxlen=max_input_lenght, padding='post', truncating='post')\n",
    "\n",
    "    #prepare data set of bots\n",
    "    #bots_list = [x for i,x in enumerate(data_train) if labels_train[i] == 0]\n",
    "    i=0\n",
    "    for label in labels_train:\n",
    "        if(label == 0):\n",
    "            bots_list.append(data_train[i])\n",
    "        i = i+1\n",
    "\n",
    "    #train - prepare data set for random tweets\n",
    "    index =0\n",
    "    tweets_list = []\n",
    "    final_label =[]\n",
    "    while(index < len(bots_list)):\n",
    "        i = random.sample(range(1, len(data_train)), 1)[0]\n",
    "        tweets_list.append(data_train[i])\n",
    "        final_label.append(1 if labels_train[i] == 0 else 0)\n",
    "        index = index + 1\n",
    "\n",
    "\n",
    "    # Encode the labels, each must be a vector with dim = num. of possible labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(final_label)\n",
    "    labels_encoded_train = le.transform(final_label)\n",
    "    categorical_labels_train = to_categorical(labels_encoded_train, num_classes=None)\n",
    "    print('Shape of train data tensor:', data_train.shape)\n",
    "    print('Shape of train label tensor:', categorical_labels_train.shape)\n",
    "    \n",
    "    return tweets_list, categorical_labels_train, le, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Test_Set(X_test, y_test, le):\n",
    "    \n",
    "    global bots_list\n",
    "    global max_input_lenght\n",
    "    global tokenizer\n",
    "    \n",
    "    # pre-process test data\n",
    "    tweets_test = [x for x in X_test]\n",
    "    labels_test = [y for y in y_test]\n",
    "\n",
    "    # convert list of tokens/words to indexes\n",
    "    sequences_test = tokenizer.texts_to_sequences(tweets_test)\n",
    "    x_test = pad_sequences(sequences_test, maxlen=max_input_lenght)\n",
    "\n",
    "    #test - prepare data set for random tweets\n",
    "    index =0\n",
    "    test_tweets_list = []\n",
    "    test_final_label =[]\n",
    "    while(index < len(bots_list)):\n",
    "        i = random.sample(range(1, len(x_test)), 1)[0]\n",
    "        test_tweets_list.append(x_test[i])\n",
    "        test_final_label.append(1 if labels_test[i] == 0 else 0)\n",
    "        index = index + 1\n",
    "\n",
    "    le.fit(test_final_label)\n",
    "    labels_encoded_test = le.transform(test_final_label)\n",
    "    categorical_labels_test = to_categorical(labels_encoded_test, num_classes=None)\n",
    "    print('Shape of test data tensor:', len(test_tweets_list))\n",
    "    print('Shape of test labels tensor:', categorical_labels_test.shape)\n",
    "    \n",
    "    return test_tweets_list, categorical_labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important  - the first and the fifth tweet is also bot and human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with pre-trained static word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(word_index):\n",
    "    global max_input_lenght\n",
    "    \n",
    "    embeddings_index = load_fasttext_embeddings()\n",
    "    embeddings_matrix = create_embeddings_matrix(embeddings_index, word_index, 300)\n",
    "    embedding_layer_static = get_embeddings_layer(embeddings_matrix, 'embedding_layer_static', max_input_lenght, trainable=False)\n",
    "    model = get_cnn_pre_trained_embeddings(embedding_layer_static, max_input_lenght)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name = 'arcii_first_version_with_two_inputs', bots_file_path = 'bots_tweets.txt' , human_file_path = 'human_tweets.txt', test_size_input = 0.3, batch_size=50, epochs=10):\n",
    "    \n",
    "    global bots_list\n",
    "    global tokenizer\n",
    "    \n",
    "    fd1 = pd.read_fwf(bots_file_path, header=None)\n",
    "    fd1.columns = ['tweets']\n",
    "    fd1['label'] = 0\n",
    "    # 0-> bots\n",
    "    \n",
    "    fd2 = pd.read_fwf(human_file_path, header=None)\n",
    "    fd2 = fd2[[0]]\n",
    "    fd2.columns = ['tweets']\n",
    "    fd2['label']=1\n",
    "    # 1-> humans\n",
    "    \n",
    "    fd = fd1.append(fd2)\n",
    "    fd.reset_index(drop = True,inplace = True)\n",
    "    \n",
    "    fd['process_tweets'] = fd['tweets']\n",
    "    fd['process_tweets'] = fd['process_tweets'].apply(text_process)\n",
    "    x_text = fd['process_tweets'].tolist()\n",
    "   \n",
    "    #split dataset\n",
    "    y = fd['label'].tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_text,y,test_size=test_size_input) # here 30% test 70% train we can change it  \n",
    "    \n",
    "    print(\"Train Samples: {}\".format(len(X_train)))\n",
    "    print(\"Test Samples : {}\".format(len(X_test)))\n",
    "    print(\"Labels       : {}\".format({x for x in y_train}))\n",
    "    \n",
    "    tokenizer = Create_Tokenizer(x_text)\n",
    "    \n",
    "    tweets_list, categorical_labels_train, le, word_index = Build_Train_Set(X_train, y_train)\n",
    "    \n",
    "    test_tweets_list, categorical_labels_test = Build_Test_Set(X_test, y_test, le)\n",
    "    \n",
    "    model = build_model(word_index)\n",
    "    with open(model_name+'.pickle', 'wb') as f:\n",
    "        pickle.dump([bots_list, max_input_lenght, tokenizer], f)\n",
    "    \n",
    "    history = model.fit([bots_list,tweets_list], categorical_labels_train, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    static = static_num2().next\n",
    "    model.save(model_name+'.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
