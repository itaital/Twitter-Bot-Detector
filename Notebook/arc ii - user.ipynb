{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARIEL\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os as os\n",
    "import io\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Reshape, Embedding, Flatten, Conv1D, Conv2D, MaxPooling1D ,MaxPooling2D, Activation\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import ManagerModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split()] #if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(user_tweet, model_name = 'arcii_first_version_with_two_inputs', threshold = 0.5):\n",
    "    model = load_model(model_name+\".h5\")\n",
    "    with open(model_name+'.pickle', 'rb') as f:\n",
    "        bots_list, max_input_lenght, tokenizer = pickle.load(f)\n",
    "    tweet_to_predict = []\n",
    "    tweet_to_predict.append(user_tweet)\n",
    "    tweet_to_predict[0] = text_process(tweet_to_predict[0])\n",
    "\n",
    "    sequences_tweet_to_predict = tokenizer.texts_to_sequences(tweet_to_predict)\n",
    "    padded_tweet_to_predict = pad_sequences(sequences_tweet_to_predict, maxlen=max_input_lenght, padding='post', truncating='post')\n",
    "\n",
    "    sum = 0\n",
    "    tweet_to_predict_list = []\n",
    "\n",
    "    for y in bots_list:\n",
    "        tweet_to_predict_list.append(padded_tweet_to_predict[0])\n",
    "    predict_unknown_tweet = model.predict([bots_list,tweet_to_predict_list], verbose=1)\n",
    "    class_predictions_unknown_tweet = [np.argmax(x) for x in predict_unknown_tweet]\n",
    "    for z in class_predictions_unknown_tweet:\n",
    "        sum+=z\n",
    "\n",
    "    if sum>=threshold*len(bots_list):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ARIEL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ARIEL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  1760/312600 [..............................] - ETA: 19:37"
     ]
    }
   ],
   "source": [
    "predict(user_tweet='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
